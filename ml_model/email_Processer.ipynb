{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset balanced and saved!\n"
     ]
    }
   ],
   "source": [
    "# Balances the dataset using Oversampling and Undersampleing \n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df = pd.read_csv(\"./Email_Dataset/Email_DataSet.csv\")\n",
    "\n",
    "applied = df[df['Label'] == 0]\n",
    "screening = df[df['Label'] == 1]\n",
    "interview = df[df['Label'] == 2]\n",
    "offer = df[df['Label'] == 3]\n",
    "rejected = df[df['Label'] == 4]  # already 69\n",
    "ads = df[df['Label'] == 5]\n",
    "other = df[df['Label'] == 6]\n",
    "\n",
    "def balance_category(category, size=69):\n",
    "    if len(category) < size:\n",
    "        return resample(category, replace=True, n_samples=size, random_state=42)\n",
    "    return category\n",
    "\n",
    "applied_balanced = balance_category(applied, 69)\n",
    "screening_balanced = balance_category(screening, 69)\n",
    "interview_balanced = balance_category(interview, 69)\n",
    "offer_balanced = balance_category(offer, 69)\n",
    "ads_balanced = balance_category(ads, 69)\n",
    "other_balanced = balance_category(other, 69)\n",
    "\n",
    "balanced_df = pd.concat([applied_balanced, screening_balanced, interview_balanced, offer_balanced, rejected, ads_balanced, other_balanced])\n",
    "\n",
    "balanced_df.to_csv('balanced_dataset.csv', index=False)\n",
    "\n",
    "print(\"Dataset balanced and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Email   Status  Label  \\\n",
      "0    Seth,\\n\\nThank you, again, for submitting your...  applied      0   \n",
      "1    Thanks for applying for the position of Softwa...  applied      0   \n",
      "2    Thank you for your interest in WorkWave! We wa...  applied      0   \n",
      "3    Thank you for applying to the Frontend Softwar...  applied      0   \n",
      "4    Seth,\\n\\nThank you, again, for submitting your...  applied      0   \n",
      "..                                                 ...      ...    ...   \n",
      "478  An account has been created for you on our com...    other      6   \n",
      "479  You only have until 11:59PM EST Tuesday, Febru...    other      6   \n",
      "480  \\nSupport resources for developers at all leve...    other      6   \n",
      "481  Thank you for your recent transaction on Steam...    other      6   \n",
      "482  \\nSupport resources for developers at all leve...    other      6   \n",
      "\n",
      "                                           Clean_Email  \n",
      "0    seth thank submitting application cognizant wa...  \n",
      "1    thanks applying position software engineer dir...  \n",
      "2    thank interest workwave wanted let know receiv...  \n",
      "3    thank applying frontend software engineer posi...  \n",
      "4    seth thank submitting application cognizant wa...  \n",
      "..                                                 ...  \n",
      "478  account created company career site access sit...  \n",
      "479  11 59pm est tuesday february 23 2021 sell eith...  \n",
      "480  support resources developers levels going litt...  \n",
      "481  thank recent transaction steam items added ste...  \n",
      "482  support resources developers levels going litt...  \n",
      "\n",
      "[483 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Humza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocesses the emails to remove stopwords, punctuations, unnecessary charactors, etc.\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_email(email):\n",
    "    email = email.lower()\n",
    "\n",
    "    email = re.sub(r'\\W', ' ', email)\n",
    "\n",
    "    email = ' '.join([word for word in email.split() if word not in stop_words])\n",
    "\n",
    "    return email\n",
    "\n",
    "csv_df = pd.read_csv(\"./Email_Dataset/balanced_dataSet.csv\")\n",
    "\n",
    "csv_df['Clean_Email'] = csv_df['Email'].apply(clean_email)\n",
    "\n",
    "csv_df.to_csv('./Email_Dataset/Cleaned_dataSet.csv', index=False)\n",
    "\n",
    "print(csv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF Matrix: (483, 1000)\n",
      "Sample of Feature Names: ['000' '000 decide' '10' '100' '100 remote' '12' '12 2019' '15' '2019'\n",
      " '2019 please' '2020' '2021' '22' '22 hours' '500' '59' '60' '60 000'\n",
      " 'abc' 'abc based']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "csv_df = pd.read_csv(\"./Email_Dataset/Cleaned_dataSet.csv\")\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), max_features=1000)\n",
    "\n",
    "X = tfidf.fit_transform(csv_df['Clean_Email'])\n",
    "\n",
    "y = csv_df['Label']\n",
    "\n",
    "print(f\"Shape of TF-IDF Matrix: {X.shape}\")\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "print(f\"Sample of Feature Names: {feature_names[:20]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
